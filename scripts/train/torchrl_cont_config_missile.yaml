#Hydra
hydra:
  job:
    chdir: True

env:
  name: 1/heading_missile
  task: "" #this is the task.
  exp_name: ${env.name}_SAC
  max_episode_steps: 1000
  seed: 42

# Logger
logger:
  backend: wandb
  mode: online
  eval_iter: 100

exp_name: cont_sac_heading
record_interval: 500
# Environment
env_name: 1/heading_missile
frame_skip: 1
from_pixels: false
reward_scaling: 1.0
init_env_steps: 0
seed: 42

# Replay Buffer
prb: 0
buffer_size: 100_000

# Optimization
utd_ratio: 1.0
gamma: 0.99
batch_size: 256
lr: 3.0e-4
weight_decay: 0.0
target_update_polyak: 0.995
target_entropy_weight: 0.2
# default is 0.98 but needs to be decreased for env
# with small action space
train:
  save_interval: 100

device: cuda:0

# optim
optim:
  utd_ratio: 1.0
  gamma: 0.99
  loss_function: l2
  lr: 3.0e-4
  weight_decay: 0.0
  batch_size: 256
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8
  anneal_lr: False

loss:
  gamma: 0.99
  mini_batch_size: 256
  ppo_epochs: 10
  gae_lambda: 0.95
  clip_epsilon: 0.2
  anneal_clip_epsilon: False
  critic_coef: 0.25
  entropy_coef: 0.01
  loss_critic_type: l2

# collector
collector:
  total_frames: 1_024_000
  init_random_frames: 0
  frames_per_batch: 2048
  device: cpu
  env_per_collector: 1
  reset_at_each_iter: False

network:
  hidden_sizes: [256, 256]
  activation: "relu"
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: "cuda:0"

# replay buffer
replay_buffer:
  size: 100000
  prb: 0 # use prioritized experience replay
  scratch_dir: ${env.exp_name}_${env.seed}
