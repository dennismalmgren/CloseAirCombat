#Hydra
hydra:
  job:
    chdir: True

env:
  name: opus/opus_training
  task: "" #this is the task.
  exp_name: P3O_OPUS
  max_episode_steps: 1000
  seed: 55
  reward_scaling: 0.01
  frame_skip: 1
  from_pixels: false

# Logger
logger:
  backend: wandb
  mode: online
  exp_name: opus_smoothing
  project: opus_smoothing
  test_interval: 48_000
  save_interval: 500_000
  num_test_episodes: 5

random:
  seed: 55

# Optimization
#target_entropy_weight: 0.2
# default is 0.98 but needs to be decreased for env
# with small action space
train:
  save_interval: 500_000

device: cuda:0
# optim
optim:
  utd_ratio: 1.0
  gamma: 0.99
  loss_function: l2
  lr_policy: 3.0e-4
  lr_q: 3.0e-4
  eps: 1.0e-4
  weight_decay: 0.00001
  mini_batch_size: 4000
  max_grad_norm: 10.0
  gae_lambda: 0.95
  clip_epsilon: 0.2
  loss_critic_type: l2
  entropy_coef: 0.01
  critic_coef: 1.0
  ppo_epochs: 5
  anneal_lr: trie
  anneal_clip_epsilon: false

# collector
collector:
  total_frames: 48_000_000
  frames_per_batch: 16000 #max_episode_steps * env_per_collector
  device: cpu
  env_per_collector: 16

network:
  policy_hidden_sizes: [256, 256, 256]
  value_hidden_sizes: [2048, 2048]
  activation: "relu"
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: "cuda:0"
  nbins: 101
  vmin: -10
  vmax: 250


# replay buffer
replay_buffer:
  size: 1_000_000
  prb: 0 # use prioritized experience replay
  scratch_dir: ${env.exp_name}_${env.seed}
